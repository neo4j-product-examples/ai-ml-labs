{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d8979e",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "In this notebook, let's explore how to use Neo4j to ground Google Generative AI.\n",
    "\n",
    "This notebook queries an SEC investment graph database using Google Vertex AI Generative AI's `code-bison` model.   We will prompt this model to convert questions in English to Cypher (Neo4j's query language) for data retrieval.  It uses the LangChain library in Python to combine the steps of generating, executing, and transforming query results into a natural language response in one smooth sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e9561",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bf94c",
   "metadata": {},
   "source": [
    "First, check to ensure you're using the `neo4j_genai` kernel with the following command. This kernel has the necessary runtime and dependencies for this notebook. If you see a different kernel, try changing the kernel to `neo4j_genai` in the upper right corner of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ff826-57ba-4bb1-a312-eb7f2aa2ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba716344",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neo4j Setup\n",
    "\n",
    "Import the Neo4j driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fed1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66325ba5",
   "metadata": {},
   "source": [
    "Provide your Neo4j credentials.  We need the DB conection URL, the username (probably `neo4j`), and your password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database credentials\n",
    "NEO4J_URI=\"<neo4j+s://xxxxx.databases.neo4j.io>\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"<password>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050e4da",
   "metadata": {},
   "source": [
    "### LangChain Setup\n",
    "\n",
    "We will import the relevant submodules from `langchain`.  We are bringing in a Neo4j Question/Answer component, and component for connecting LangChain to the Neo4j graph to be queried, the VertexAI LLM object, and a utility for creating prompt templates that will aid in Cypher query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a76b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain, LLMChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea81aba-c3e5-435b-9a9a-86d0a65da844",
   "metadata": {},
   "source": [
    "## Base Example Without Grounding\n",
    "\n",
    "Before grounding with the Neo4j, let's setup up a baseline that just uses an LLM to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d45b7-1406-4454-8811-67a2b351c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_chain = VertexAI(model_name=\"text-bison@001\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6d342-e9dc-4533-955e-601e181011ae",
   "metadata": {},
   "source": [
    "We can now ask a simple finance question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933a2d9-53fd-41df-8b5a-248885b0dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_response = base_chain(\"\"\"What are the top 10 investments for Blackrock?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621825b-39bd-4e15-8dc0-3e2308bab349",
   "metadata": {},
   "source": [
    "While this answer looks reasonable, we have no real way to know how the LLM came it with it, or where it was sourced from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f64a21-8347-4eec-a83a-8044f824a3d8",
   "metadata": {},
   "source": [
    "Here is a more complicated example where we expect the LLM to understand some more specific terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43c396-39d5-4511-bc3d-a56a1880afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_response = base_chain(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cf10e-45f4-427d-bda6-a7ae4c706d92",
   "metadata": {},
   "source": [
    "In this case, it looks like the LLM understands the ubiquitous acronym \"FAANG\" but, unsurprisingly, the results indicate it doesn't understand \"manager\" within the context of our data model.  In your use case, you may have lots of specific terminology/ontology like this that you would need a chatbot to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a087cb5-f99d-4c7c-bdac-14ec8cd411cb",
   "metadata": {},
   "source": [
    "## Grounding LLMs with Neo4j\n",
    "\n",
    "Now let's create a chatbot that is grounded with Neo4j. Below is the pattern we will follow with LangChain:\n",
    "\n",
    "![](images/langchain-neo4j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044359a",
   "metadata": {},
   "source": [
    "### Cypher Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd6ec8",
   "metadata": {},
   "source": [
    "We have to use a prompt template that: \n",
    "1. Clearly states what schema to use \n",
    "2. Provides principles the chatbot should follow in generating responses\n",
    "3. Demonstrates few-shot examples to help the chatbot be more accurate in its query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23987ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt/template \n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Please do not use same variable names for different nodes and relationships in the query.\n",
    "4. Use only Nodes and relationships mentioned in the schema\n",
    "5. Always enclose the Cypher output inside 3 backticks\n",
    "6. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "7. Candidate node is synonymous to Manager\n",
    "8. Always use aliases to refer the node in the query\n",
    "9. 'Answer' is NOT a Cypher keyword. Answer should never be used in a query.\n",
    "10. Please generate only one Cypher query per question. \n",
    "11. Cypher is NOT SQL. So, do not mix and match the syntaxes.\n",
    "12. Every Cypher query always starts with a MATCH keyword.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "Samples:\n",
    "Question: Which fund manager owns most shares? What is the total portfolio value?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, sum(distinct o.shares) as ownedShares, sum(o.value) as portfolioValue ORDER BY ownedShares DESC LIMIT 10\n",
    "\n",
    "Question: Which fund manager owns most companies? How many shares?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, count(distinct c) as ownedCompanies, sum(distinct o.shares) as ownedShares ORDER BY ownedCompanies DESC LIMIT 10\n",
    "\n",
    "Question: What are the top 10 investments for Vanguard?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" RETURN c.companyName as Investment, sum(DISTINCT o.shares) as totalShares, sum(DISTINCT o.value) as investmentValue order by investmentValue desc limit 10\n",
    "\n",
    "Question: What other fund managers are investing in same companies as Vanguard?\n",
    "Answer: MATCH (m1:Manager) -[:OWNS]-> (c1:Company) <-[o:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) contains \"vanguard\" AND elementId(m1) <> elementId(m2) RETURN m2.managerName as manager, sum(DISTINCT o.shares) as investedShares, sum(DISTINCT o.value) as investmentValue ORDER BY investmentValue LIMIT 10\n",
    "\n",
    "Question: What are the top investors for Apple?\n",
    "Answer: MATCH (m1:Manager) -[o:OWNS]-> (c1:Company) WHERE toLower(c1.companyName) contains \"apple\" RETURN distinct m1.managerName as manager, sum(o.value) as totalInvested ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are the other top investments for fund managers investing in Apple?\n",
    "Answer: MATCH (c1:Company) <-[:OWNS]- (m1:Manager) -[o:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND elementId(c1) <> elementId(c2) RETURN DISTINCT c2.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are the top investors in the last 3 months?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE date() > o.reportCalendarOrQuarter > o.reportCalendarOrQuarter - duration({{months:3}}) RETURN distinct m.managerName as manager, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are top investments in last 6 months for Vanguard?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:6}}) RETURN distinct c.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Who are Apple's top investors in last 3 months?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(c.companyName) contains \"apple\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:3}}) RETURN distinct m.managerName as investor, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Which fund manager under 200 million has similar investment strategy as Vanguard?\n",
    "Answer: MATCH (m1:Manager) -[o1:OWNS]-> (:Company) <-[o2:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) CONTAINS \"vanguard\" AND elementId(m1) <> elementId(m2) WITH distinct m2 AS m2, sum(distinct o2.value) AS totalVal WHERE totalVal < 200000000 RETURN m2.managerName AS manager, totalVal*0.000001 AS totalVal ORDER BY totalVal DESC LIMIT 10\n",
    "\n",
    "Question: Who are common investors in Apple and Amazon?\n",
    "Answer: MATCH (c1:Company) <-[:OWNS]- (m:Manager) -[:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND toLower(c2.companyName) CONTAINS \"amazon\" RETURN DISTINCT m.managerName LIMIT 50\n",
    "\n",
    "Question: What are Vanguard's top investments by shares for 2023?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.value) AS totalValue ORDER BY totalValue DESC LIMIT 10\n",
    "\n",
    "Question: What are Vanguard's top investments by value for 2023?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.shares) AS totalShares ORDER BY totalShares DESC LIMIT 10\n",
    "\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e46b3",
   "metadata": {},
   "source": [
    "Now let’s create a LangChain prompt template.  \n",
    "\n",
    "This template defines the parameter inputs for the prompt sent to the Cypher generation bot.  In our example, the inputs will be `schema` and `question`.  The question comes from the end user.  The LangChain `GraphCypherQAChain` automatically inserts the schema via a built-in method to `Neo4jGraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86addda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\",\"question\"], validate_template=True, template=CYPHER_GENERATION_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e2e7d",
   "metadata": {},
   "source": [
    "We need to connect to the graph via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph object from Langchain\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI, \n",
    "    username=NEO4J_USERNAME, \n",
    "    password=NEO4J_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1263e2",
   "metadata": {},
   "source": [
    "We are defining our `chain` object, which combines Neo4j Q/A and Vertex AI's [code-bison](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-generation) LLM.  When the user gives a query, it first goes through `GraphCypherQAChain`, which generates a Cypher query according to the rules in our prompt above.  That result set then goes to the `VertexAI` step of the chain, where the LLM is given the Neo4j result set and instructed to roll it into a natural language response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec96b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the chain from VertexAI langchain object\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    VertexAI(model_name='code-bison@001',\n",
    "            max_output_tokens=2048,\n",
    "            temperature=0.0), graph=graph, verbose=True,\n",
    "            cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1590f",
   "metadata": {},
   "source": [
    "Below we have a few examples of how we can get answers from the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed32736-43f8-4c9c-b90e-69932461bbd3",
   "metadata": {},
   "source": [
    "### Why Ground Your LLM?\n",
    "\n",
    "Recall our base example where we asked for the top 10 Blackrock investments?  We got an answer that looked like it may be reasonable, but we couldn't validate it or track sources.  We also asked what managers own FAANG stocks, and for that, we unsurprisingly received the wrong answers for our use case.\n",
    "\n",
    "Let's try again grounding with Neo4j. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = chain(\"\"\"What are the top 10 investments for Blackrock?\"\"\")\n",
    "print(f\"Final answer: {r2['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65434f3-f818-46ca-9fc8-0daa4a77942b",
   "metadata": {},
   "source": [
    "Notice that this answer is different from our base example, and this time we have the Cypher logic used to obtain the answer from our database. This means that we can trace back how we came up with this answer and make any adjustments to our database or prompt if we need to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7da574-8c23-4c78-b35f-bb4e86cdb291",
   "metadata": {},
   "source": [
    "Now lets try the FAANG question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d1464-7202-4d66-af52-3a6ffe8668d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = chain(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {r3['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0686244-bb6b-4231-aebf-70b3c777ef25",
   "metadata": {},
   "source": [
    "Here again, we notice the traceability with Cypher, and because we engineered our prompt to include our schema, it understood what “manager” means in the context of our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d119d1-5b84-4665-985c-42eeeca49779",
   "metadata": {},
   "source": [
    "### Why Ground your LLM with Neo4j?\n",
    "\n",
    "There are 3 primary reasons to ground your LLM with Neo4j specifically:\n",
    "1. __Grounding for more complex question handling__: Multi-hop knowledge retrieval across connected data. Connections between data points are calculated before query time. \n",
    "2. __Enterprise reliability and security__: Fine-grained security so the chatbot only accesses information the user has permission to. Autonomous clustering for horizontal scaling.  Fully managed service in the cloud through Aura. \n",
    "3. __Performance__: fast queries with high concurrency for many users.\n",
    "\n",
    "We can explore point 1 with more complex questions below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abffb7-fc75-4d76-8d11-5ec1427f5a7c",
   "metadata": {},
   "source": [
    "A question requiring ~4 hops (would be joins in the relational world).  Having a knowledge graph with relationships calculated before query time allows us to answer the question quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r4 = chain(\"\"\"What are other top investments for fund managers investing in AstraZeneca?\"\"\")\n",
    "print(f\"Final answer: {r4['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c1f42-d475-4075-9168-61f65fe6284f",
   "metadata": {},
   "source": [
    "Combine also with property conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ae99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r5 = chain(\"\"\"Which fund manager under 200 million has similar investment strategy as Blackrock\"\"\")\n",
    "print(f\"Final answer: {r5['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbca6ab-01d6-4697-a58f-7978567ff66b",
   "metadata": {},
   "source": [
    "and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588955da",
   "metadata": {},
   "outputs": [],
   "source": [
    "r6 = chain(\"\"\"Please get me 10 common investors between Tesla and Microsoft\"\"\")\n",
    "print(f\"Final answer: {r6['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b038e",
   "metadata": {},
   "source": [
    "## Grounded Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c246c",
   "metadata": {},
   "source": [
    "Now we will use Gradio to deploy a chat interface with our chain behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b608a",
   "metadata": {},
   "source": [
    "The below code deploys a Gradio application.  You can access the app via a local URL. A publicly sharable URL is also provided (sharable for 3 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import typing_extensions\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "llm = VertexAI(model_name='code-bison@001', temperature=0.2)\n",
    "agent_chain = chain\n",
    "\n",
    "def chat_response(input_text,history):\n",
    "    try:\n",
    "        return agent_chain.run(input_text)\n",
    "    except:\n",
    "        # a bit of protection against exposed error messages\n",
    "        # we could log these situations in the backend to revisit later in development\n",
    "        return \"I'm sorry, there was an error retrieving the information you requested.\"\n",
    "\n",
    "interface = gr.ChatInterface(fn = chat_response,\n",
    "                             title = \"Investment Chatbot\",\n",
    "                             description = \"powered by Neo4j\",\n",
    "                             theme = \"soft\",\n",
    "                             chatbot = gr.Chatbot(height=500),\n",
    "                             undo_btn = None,\n",
    "                             clear_btn = \"\\U0001F5D1 Clear chat\",\n",
    "                             examples = [\"Who are Tesla's top investors in last 3 months?\",\n",
    "                                         \"What are the top 10 investments for Blackrock?\",\n",
    "                                         \"Which manager owns FAANG stocks?\",\n",
    "                                         \"What are other top investments for fund managers investing in Exxon?\",\n",
    "                                         \"What are Vanguard's top investments by value for 2023?\",\n",
    "                                         \"Who are the common investors between Tesla and Microsoft?\"])\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e27bc-e586-467b-8ace-6d3893d2fad1",
   "metadata": {},
   "source": [
    "## Note on Fine-Tuning for Cypher Generation\n",
    "\n",
    "We encourage you to use Vertex AI Codey family models with a schema, few-shot examples, and precise prompt engineering for Cypher generation. However, if that still doesn't provide an appropriate level of quality, or you need your LLM to improve accuracy on a more specific task area, you can try fine-tuning.\n",
    "\n",
    "Fine-tuning is the process of taking a foundational model and making precise changes to improve its performance for a specific, narrower task. It works by taking in training data containing many examples of your specific task and using it to update or add additional parameters in a new version of the model.\n",
    "\n",
    "The total training time generally takes more than an hour. The tuned adapter model is going to stay within your tenant, and your training data will not be used to train the base model, which is frozen. Tuning runs on GCP's TPU infrastructure that is optimized to run ML workloads.\n",
    "\n",
    "The training data should be structured as a supervised training set, where each row contains input text and desired, resulting, Cypher query. Vertex AI expects you to adhere to the below format in a `jsonl` file.\n",
    "\n",
    "```\n",
    "{\"input_text\": \"MY_INPUT_PROMPT\", \"output_text\": \"CYPHER_QUERY\"}\n",
    "```\n",
    "You can find more about fine-tuning in the [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5197afd",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92158fb",
   "metadata": {},
   "source": [
    "In this notebook, we went through the steps of connecting a LangChain agent to a Neo4j database and using it to generate Cypher queries in response to user requests via LLMs on Vertex AI, thus grounding the LLM with a knowledge graph.\n",
    "\n",
    "While we used the `code-bison` model here, this approach can be generalized to other Vertex AI LLMs.  This process can also be augmented with additional steps around the generation chain to customize the experience for specific use cases.  \n",
    "\n",
    "The critical takeaway is the importance of Neo4j Knowledge Graph as a grounding database to \n",
    "- anchor your chatbot to reality as it generates responses and \n",
    "- to enable your LLM to provide answers enriched with relevant enterprise data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j_genai (Local)",
   "language": "python",
   "name": "local-neo4j_genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
