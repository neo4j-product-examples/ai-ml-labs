{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ae6874-7585-4a0b-848c-965f639def41",
   "metadata": {},
   "source": [
    "# Load 10K Filings\n",
    "\n",
    "In this notebook, we add 10K filings with embeddings to our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbadcd3e-3306-41bf-986a-bb690b2f1a7b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, check to ensure you're using the `neo4j_genai` kernel with the following command. This kernel has the necessary runtime and dependencies for this notebook. If you see a different kernel, try changing the kernel to `neo4j_genai` in the upper right corner of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a58389-bf6e-4304-bf49-3d3d06a5732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.path.basename(sys.executable.replace(\"/bin/python\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7c539-68b5-4126-990e-60c85b84fafa",
   "metadata": {},
   "source": [
    "Now import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7aeea-5d5d-4cbc-a454-9708de3a3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from string import Template\n",
    "import pandas as pd\n",
    "\n",
    "# Neo4j\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "# Google Cloud\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e367a7-5120-416f-ade9-2be7257ef069",
   "metadata": {},
   "source": [
    "## Get 10K Documents with Embeddings\n",
    "You can skip this step if you ran the part 0 notebook to generate the embeddings.  This downloads pre-run documents and embeddings from 10K Item 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a6543-65d4-4f50-a34a-744f9c9c21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this if you ran part 0\n",
    "\n",
    "storage_client = storage.Client()\n",
    "(storage_client\n",
    " .bucket('neo4j-datasets')\n",
    " .blob('form10k/form10k-doc-embeddings.csv')\n",
    " .download_to_filename('form10k-doc-embeddings.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446e651-9134-4634-a527-4f2745c7d623",
   "metadata": {},
   "source": [
    "## Loading 10K Documents with Embeddings into Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca3c83-fdc8-4d30-bacd-4c0b0328655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df = pd.read_csv('form10k-doc-embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b21e9f-62b1-4ca8-89cf-2ff2a3d181da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to transform textEmbeddings to a list instead of String.  json.loads should do the trick\n",
    "emb_df['textEmbedding'] = emb_df['textEmbedding'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eee976-129a-4813-887a-4b87e349642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e9200-915b-4fed-9b58-0db4174120ba",
   "metadata": {},
   "source": [
    "Provide your Neo4j credentials.  We need the DB conection URL, the username (probably `neo4j`), and your password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fffd14-e6bb-4843-90e9-d1d4e7cef3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database credentials\n",
    "NEO4J_URI= \"<neo4j+s://xxxxx.databases.neo4j.io>\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD= \"<password>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e2ef1-f5bd-4561-87e7-a014b8af2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=True\n",
    ")\n",
    "gds.set_database('neo4j')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b324108-6bc9-45d6-903f-0868ac036298",
   "metadata": {},
   "source": [
    "Remember to create indexes. We will be merging 10K documents by `companyName`. In a production setting, we would want to use a better identifier here (like we did with cusip for Company) However, this should suffice for our intents and purposes as we are just getting acquainted to learning about semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa827ed2-4b31-4b8c-ad63-1e3f210a5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.run_cypher('CREATE INDEX company_name IF NOT EXISTS FOR (n:Company) ON (n.companyName)')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_document_id IF NOT EXISTS FOR (n:Document) REQUIRE (n.documentId) IS NODE KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab46965-a5d9-42e6-a2fd-424f1d22fb6a",
   "metadata": {},
   "source": [
    "Due to the size of the documents we will want to transform the dataframe into a list of dict that we can chunk up and insert via parameterized query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cb172-48da-4742-9ba6-6c6bb20c6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_entries = emb_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d07e4-fe0b-4306-922a-22307ee3cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(xs, n=5):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e8a3a-137e-464c-b901-fba2b023178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "total = len(emb_entries)\n",
    "count = 0\n",
    "for d in chunks(emb_entries, 100):\n",
    "    gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MATCH(c:Company {companyName:record.companyName})\n",
    "    MERGE(b:Document {documentId:record.contextId})\n",
    "    SET b.documentType='FORM_10K_ITEM1', b.seqId = record.seqId, b.textEmbedding = record.textEmbedding, b.text = record.text\n",
    "    MERGE(c)-[:HAS]->(b)\n",
    "    RETURN count(b) as cnt\n",
    "    ''', params = {'records':d})\n",
    "    count += len(d)\n",
    "    print(f'loaded {count} of {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af24f67-f6c9-4f1c-9ae2-b499e173a28b",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e07f86-3889-4a46-baf4-6e8e5fb53c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check node count\n",
    "gds.run_cypher('MATCH(doc:Document) RETURN count(doc)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66464a3-cb8e-4312-9ec1-38b16bacd3ca",
   "metadata": {},
   "source": [
    "Note that we were only getting 10K docs for a minority of companies. It should be fine for this, but in a more rigorous setting, you may want to try and pull more.  There are likely a few factors attributing to this. \n",
    "\n",
    "1. We used company names to search EDGAR which resulted in many misses and dups which were discarded. In a more rigorous setting, we would investigate other endpoints and use more parsing to extract EDGAR cik keys for exact matching companies when pulling forms.\n",
    "\n",
    "2. Company names are not consistent across form13 filings, so even if we successfully pull on one version of a company name, we may not be able to merge it into the graph via the one company name represented there. \n",
    "\n",
    "3. Not all companies in the dataset are obligated to file 10Ks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bf117-5285-4928-88e8-1da35025e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check count and percentage of companies with 10K docs.  Note it is the minority\n",
    "gds.run_cypher('''\n",
    "MATCH(b:Company)\n",
    "WITH b, count{(b)-[:HAS]->(d:Document)} AS docCount\n",
    "WITH count(b) AS total, sum(toInteger(docCount > 0)) AS numWithDocs\n",
    "RETURN total, numWithDocs, round(100*toFloat(numWithDocs)/toFloat(total), 2) As PercWithDocs\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75281ba-3bd8-43fb-adda-e61b1410036f",
   "metadata": {},
   "source": [
    "Note that there are duplicate names.  For our purposes here, we will treat it as entity resolution, meaning that we treat companies with the same name as belonging to the same overarching entity for semantic search. In a more rigorous setting, we would need to disambiguate with the CUSIP or other EDGAR keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e242a-e008-473b-9dcc-7fc08ab928fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show duplicates via HAS relationship\n",
    "gds.run_cypher('''\n",
    "MATCH(b:Company)\n",
    "RETURN count(b) AS totalCompanies, count(DISTINCT b.companyName) AS uniqueCompanyNames\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd25c1f-3a44-4a5f-800b-59fb3ac72930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "neo4j_genai (Local)",
   "language": "python",
   "name": "local-neo4j_genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
